{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e8da3c-708f-4d00-9949-cb504e03e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_final_test = 'data_final_test.csv'\n",
    "data_final_train = 'data_final_train.csv'\n",
    "\n",
    "# Read the Excel file\n",
    "data_final_test = pd.read_csv(data_final_test)\n",
    "data_final_train = pd.read_csv(data_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb2124e-c688-47e7-aa77-91ba38a6d612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa110b7a76e4f979b2f7e5ab97f191f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e08040-63e4-4b55-983a-cfe79d54533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095e7691-6b74-4c49-8f2f-668464f9b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_pandas(data_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f687ebe-65ae-4285-8f65-489e843b5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"/datadrive1/llama/llama-2-13b-chat-hf\"\n",
    "use_flash_attention = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63f786e-b303-4f08-862c-305891c1df9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a2c3fb7a4c47bda8b6793608dd91b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# quantization_config_loading = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=False\n",
    "# )\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    use_flash_attention_2=use_flash_attention,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#                                 model_id,\n",
    "#                                 quantization_config=quantization_config_loading,\n",
    "#                                 device_map=\"auto\"\n",
    "#                             )\n",
    "\n",
    "# Move the model to GPU slots 3 and 4\n",
    "# model = model.to(\"cuda:2,3\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20af559e-d0f2-47cf-acdd-251f94bb269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a801a46-55e3-43e4-895c-6ca634297012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb0386c-08d6-404f-9a8b-f82b7ace7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"LLaMa2_13B_Chat-finetuned-New_annotations_v10\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    # max_grad_norm=0.3,\n",
    "    # warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=False,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39c61c1f-b5e7-4b4c-a5ae-a2f2f3ee5085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7703eee35e5d4e5e8dc5c13e2c200a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 1024 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    # formatting_func=format_prompt,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bedaaa81-9c76-4bd7-a57d-72fcd521033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 41:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.655500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /datadrive1/llama/llama-2-13b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /datadrive1/llama/llama-2-13b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /datadrive1/llama/llama-2-13b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=459, training_loss=0.9714628398288568, metrics={'train_runtime': 2521.6308, 'train_samples_per_second': 1.45, 'train_steps_per_second': 0.182, 'total_flos': 2.9112310076276736e+17, 'train_loss': 0.9714628398288568, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_model = \"LLaMa2_13B_Chat-finetuned-New_annotations_v10\"\n",
    "# Train\n",
    "trainer.train()\n",
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5762c894-bcab-41ac-b425-3594ca224695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anshul/.virtualenvs/DS_app/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /datadrive1/llama/llama-2-13b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41ec20-77bd-4b44-b317-f35196ed791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporating our adapter weights into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1934d3e2-02ab-4c76-beb8-6fbea6e95e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada2d0ca253b4deb96b47ea67fcef7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('merged_LLaMa2_13B_Chat-finetuned_New_annotations_v10_vllm/tokenizer_config.json',\n",
       " 'merged_LLaMa2_13B_Chat-finetuned_New_annotations_v10_vllm/special_tokens_map.json',\n",
       " 'merged_LLaMa2_13B_Chat-finetuned_New_annotations_v10_vllm/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    training_arguments.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_LLaMa2_13B_Chat-finetuned_New_annotations_v10_vllm\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_LLaMa2_13B_Chat-finetuned_New_annotations_v10_vllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43089c88-7ac5-452c-bf80-3a72ed48fa39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
