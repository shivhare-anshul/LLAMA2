{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5a270-4274-4915-8e9a-676248f03cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "def extract_text(results):\n",
    "    pattern = re.compile(r'Response\\s*:\\s*({.*?})', re.DOTALL)\n",
    "    extracted_texts = []\n",
    "    count = 0\n",
    "    for text in results:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted_texts.append(match.group(1))\n",
    "        else:\n",
    "            extracted_texts.append(\"\")\n",
    "            count += 1\n",
    "    print(f\"Pattern not found for {count} results.\")\n",
    "    return extracted_texts\n",
    "\n",
    "def calculate_bleu_score(machine_results, reference_texts):\n",
    "    bleu_score = corpus_bleu([[ref.split()] for ref in reference_texts], [gen.split() for gen in machine_results])\n",
    "    print(f'BLEU Score: {bleu_score}')\n",
    "\n",
    "def calculate_rouge_scores(generated_answers, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n",
    "    for gen, ref in zip(generated_answers, ground_truth):\n",
    "        scores = scorer.score(gen, ref)\n",
    "        total_rouge1 += scores['rouge1'].fmeasure\n",
    "        total_rouge2 += scores['rouge2'].fmeasure\n",
    "        total_rougeL += scores['rougeL'].fmeasure\n",
    "    average_rouge1 = total_rouge1 / len(generated_answers)\n",
    "    average_rouge2 = total_rouge2 / len(generated_answers)\n",
    "    average_rougeL = total_rougeL / len(generated_answers)\n",
    "    print(f'Average ROUGE-1: {average_rouge1}')\n",
    "    print(f'Average ROUGE-2: {average_rouge2}')\n",
    "    print(f'Average ROUGE-L: {average_rougeL}')\n",
    "\n",
    "def calculate_bert_score(generated_answers, ground_truth):\n",
    "    scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "    P, R, F1 = scorer.score(generated_answers, ground_truth)\n",
    "    # for i, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    #     print(f\"Pair {i + 1} - BERTScore Precision: {p.mean():.4f}, Recall: {r.mean():.4f}, F1: {f1.mean():.4f}\")\n",
    "    avg_precision = sum(p.mean() for p in P) / len(P)\n",
    "    avg_recall = sum(r.mean() for r in R) / len(R)\n",
    "    avg_f1 = sum(f1.mean() for f1 in F1) / len(F1)\n",
    "    print(f\"\\nAverage BERTScore - Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c58ff8-280e-4da6-a441-6fa47c37dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('data_final_test.csv')\n",
    "print(len(df))\n",
    "machine_results_Finetuned = list(df[\"Result_Finetuned\"])\n",
    "machine_results_Finetuned_witout_vllm_weights = list(df[\"Result_Finetuned_without_vllm_weights\"])\n",
    "machine_results_Non_Finetuned = list(df[\"Result_Non_Finetuned\"])\n",
    "reference_texts = list(df[\"Ground_Truth\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38837a-3902-4562-9088-750095c8198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_results_Finetuned[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df7459-b3ac-4700-9b23-dfa5e02e3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_results_Non_Finetuned[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21d935-0474-47e1-8963-029f379b66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3934537-2dce-4397-a17a-adf9eac30f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from results\n",
    "machine_results_Finetuned_Copy = extract_text(machine_results_Finetuned)\n",
    "machine_results_Finetuned_witout_vllm_weights_Copy = extract_text(machine_results_Finetuned_witout_vllm_weights)\n",
    "machine_results_Non_Finetuned_Copy = extract_text(machine_results_Non_Finetuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1accbe-4fc5-464c-a4c1-ffcac556356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "print(\"BLEU Score for Finetuned:\")\n",
    "calculate_bleu_score(machine_results_Finetuned_Copy, reference_texts)\n",
    "print(\"BLEU Score for Finetuned without vllm weights:\")\n",
    "calculate_bleu_score(machine_results_Finetuned_witout_vllm_weights_Copy, reference_texts)\n",
    "print(\"BLEU Score for Non-Finetuned:\")\n",
    "calculate_bleu_score(machine_results_Non_Finetuned_Copy, reference_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603dda2a-08fb-4b5b-a1d2-6e3beb8d797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "print(\"ROUGE Scores for Finetuned:\")\n",
    "calculate_rouge_scores(machine_results_Finetuned_Copy, reference_texts)\n",
    "print(\"ROUGE Scores for Finetuned without vllm weights:\")\n",
    "calculate_rouge_scores(machine_results_Finetuned_witout_vllm_weights_Copy, reference_texts)\n",
    "print(\"ROUGE Scores for Non-Finetuned:\")\n",
    "calculate_rouge_scores(machine_results_Non_Finetuned_Copy, reference_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13495850-8e66-4dbe-b029-56ad44631484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERTScore\n",
    "print(\"BERTScores for Finetuned:\")\n",
    "calculate_bert_score(machine_results_Finetuned_Copy, reference_texts)\n",
    "\n",
    "print(\"BERTScores for Finetuned without vllm weights:\")\n",
    "calculate_bert_score(machine_results_Finetuned_witout_vllm_weights_Copy, reference_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c973f9e-2813-490b-acb1-9462b67893a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERTScores for Non-Finetuned:\")\n",
    "calculate_bert_score(machine_results_Non_Finetuned_Copy, reference_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea3c0e-d622-491e-9e15-3333f3bb7532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98fe8-bf09-4ff4-b21e-7ab65b8ab25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24875502-9283-455c-a892-8af63e7c7ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2b0f0-6041-45f1-b2c4-ea6ed98be656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
